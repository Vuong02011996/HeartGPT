# 1. Introduction

# 2. The Pre-Trained Transformer Models
## Tokenization and Embedding
## Multi-Head Masked Self Attention and the Transformer Block
## Architecture
## Pre-Training
## Fine-Tuning
    ### Atrial Fibrillation
    ### PPG Beat Detection
## Cross Entropy Loss and Next Token Prediction vs Mean Squared Error

# 3. Generative Model Evaluation

# 4. Interpretability of the Pre-Trained Models
## Interpretability of aggregate model attention
## Vector similarities between points of interest
## Attention Maps of Individual Attention Heads

# 5. Fine-Tuning Results
## Change in Attention when Screening For Atrial Fibrillation
## Beat Detection and Signal Quality Estimation in PPG

# 6. Conclusion